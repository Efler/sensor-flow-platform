sensor-flow-platform

1) dev-setup
2) configuring dev + prod (theoretically)


docker build -t spark-job-launcher -f Dockerfile-job-launcher .
docker run --rm -it --network=sensor-flow-platform-network spark-job-launcher bash


{
  "device_id": "device-001",
  "metric_name": "voltage",
  "metric_value": 22.4,
  "src_timestamp": "2025-04-09T14:32:10Z",
  "signature": "b2cbdea117b8d294281d50061e51ea55eccc0e6713cb1f26b63ceb0e9387e114"
}
{
  "device_id": "device-001",
  "metric_name": "voltage",
  "metric_value": 22.4,
  "src_timestamp": "2025-04-09T14:32:10.000Z",
  "thresholds": {
    "lower": 118.69388683163346,
    "upper": 249.65966692414833
  },
  "alert_type": "BROKEN_LOWER_THRESHOLD"
}

api-token: meow-meow!



docker exec -it spark-master bash

\
spark-submit \
--class org.eflerrr.sfp.sparkjobs.smth.Main \
--master spark://sensor-flow-platform-spark-master-1 \
--deploy-mode cluster \
--executor-memory 512m \
--total-executor-cores 2 \
--supervise \
./alerter-fat.jar


spark-submit \
  --class org.eflerrr.sfp.sparkjobs.visualizer.Main \
  --master spark://spark-master-1:7077 \
  --deploy-mode cluster \
  --executor-memory 512m \
  --total-executor-cores 5 \
  --conf "spark.driver.extraJavaOptions=-Dlog4j2.rootLogger.level=WARN" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j.rootLogger=WARN" \
  --supervise \
  ./../../spark-jobs/core-fat.jar




spark-submit \
  --class org.eflerrr.sfp.sparkjobs.visualizer.Main \
  --master spark://spark-master-2:7077 \
  --deploy-mode cluster \
  --executor-memory 512m \
  --total-executor-cores 5 \
  --conf spark.hadoop.fs.s3a.endpoint=minio-proxy:9990 \
  --conf spark.hadoop.fs.s3a.access.key=minioadmin \
  --conf spark.hadoop.fs.s3a.secret.key=minioadmin \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  --conf spark.sql.adaptive.enabled=false \
  --supervise \
  ./../../spark-jobs/core-fat.jar

  --supervise \
  --conf "spark.driver.extraJavaOptions=-Dlog4j2.rootLogger.level=WARN" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j.rootLogger=WARN" \




spark-submit --class org.eflerrr.sfp.sparkjobs.visualizer.Main --master spark://spark-master-2:7077 --deploy-mode cluster --executor-memory 512m --total-executor-cores 5 --conf "spark.driver.extraJavaOptions=-Dlog4j2.rootLogger.level=WARN" --conf "spark.executor.extraJavaOptions=-Dlog4j.rootLogger=WARN" --supervise ./../../spark-jobs/core-fat.jar

spark-submit --class org.eflerrr.sfp.sparkjobs.visualizer.Main --master spark://spark-master-2:7077 --deploy-mode cluster --executor-memory 512m --total-executor-cores 5 --supervise ./../../spark-jobs/core-fat.jar


kafka-topics.sh --create --bootstrap-server broker-1:9092 --replication-factor 2 --partitions 3 --topic sensors-data
--conf spark.hadoop.fs.s3a.endpoint=http://minio1:9000,http://minio2:9000,http://minio3:9000,http://minio4:9000 \
  --conf spark.hadoop.fs.s3a.access.key=admin \
  --conf spark.hadoop.fs.s3a.secret.key=password \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem


 === grafana ===
SELECT DISTINCT device_id FROM sensors_metrics ORDER BY device_id;

SELECT src_timestamp AS time, metric_value
FROM sensors_metrics
WHERE device_id = '$var_device_id' AND metric_name = 'temperature'
ORDER BY src_timestamp;




---------- thresholder ----------
device_id | metric_name | threshold_type | threshold
---
device-001 | voltage | lower | 118.873887
device-001 | voltage | upper | 250.109667
--
device-002 | voltage | lower | 110.896701
device-002 | voltage | upper | 259.144512
--
device-003 | current | lower | 51.313285
device-003 | current | upper | 130.561020

==
118.853887
250.059667
110.876701
259.094512
51.303285
130.53102
-----------------------------------